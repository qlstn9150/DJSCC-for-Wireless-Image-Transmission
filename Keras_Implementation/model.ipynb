{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb4771f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, Layer, Input, Conv2DTranspose, UpSampling2D, Cropping2D\n",
    "#from keras.optimizers import Adam\n",
    "from tensorflow.python.keras.optimizers import *\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.models import Model\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e10638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizationNoise(Layer): #from keras.layers\n",
    "    def __init__(self, snr_db_def = 20, P_def=1, name='NormalizationNoise', **kwargs): #kwargs: 딕셔너리 형태로 함수에 전달\n",
    "        self.snr_db = K.variable(snr_db_def, name='SNR_db')\n",
    "        self.P = K.variable(P_def, name='Power')\n",
    "        self.name = name ###왜 K모듈 안썼나? \n",
    "        super(NormalizationNoise, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, z_tilta): \n",
    "        #레이어 이름별로 범위 나눔\n",
    "        with tf.name_scope('Normalization_Layer'):\n",
    "\n",
    "            z_tilta = tf.dtypes.cast(z_tilta, dtype='complex128', name='ComplexCasting')+1j ###1j??  #복소수형으로 변환\n",
    "            lst = z_tilta.get_shape().as_list() #행렬 차원을 리스트로 표현\n",
    "            lst.pop(0) #0번째 요소\n",
    "\n",
    "            #computing channel dimension 'k' as the channel bandwidth. #주파수 범위\n",
    "            k = np.prod(lst, dtype='float32') #배열 원소들 간의 곱을 실수형으로\n",
    "\n",
    "            #calculating conjugate transpose of z_tilta\n",
    "            #전치행렬로 만들어서 복소수의 켤레 반환 ###0,2,1,3 모르겠음???\n",
    "            z_conjugateT = tf.math.conj(tf.transpose(z_tilta, perm=[0,2,1,3], name='transpose'), name='z_ConjugateTrans')\n",
    "            \n",
    "            #Square root of k and P\n",
    "            sqrt1 = tf.dtypes.cast(tf.math.sqrt(k*self.P, name='NormSqrt1'), dtype='complex128',name='ComplexCastingNorm')\n",
    "            sqrt2 = tf.math.sqrt(z_conjugateT*z_tilta, name='NormSqrt2')#Square root of z_tilta* and z_tilta.\n",
    "            div = tf.math.divide(z_tilta,sqrt2, name='NormDivision')\n",
    "            \n",
    "            #calculating channel input\n",
    "            z = tf.math.multiply(sqrt1,div, name='Z')   \n",
    "            \n",
    "        with tf.name_scope('PowerConstraint'):     \n",
    "            ############# Implementing Power Constraint ##############\n",
    "            z_star = tf.math.conj(tf.transpose(z, perm=[0,2,1,3], name='transpose_Pwr'), name='z_star')\n",
    "            prod = z_star*z \n",
    "            real_prod = tf.dtypes.cast(prod , dtype='float32', name='RealCastingPwr')\n",
    "            pwr = tf.math.reduce_mean(real_prod)\n",
    "            cmplx_pwr = tf.dtypes.cast(pwr, dtype='complex128', name='PowerComplexCasting')+1j\n",
    "            pwr_constant = tf.constant(1.0, name ='PowerConstant')\n",
    "            # Z: satisfies 1/kE[z*z] <= P, where P=1\n",
    "            Z = tf.cond(pwr>pwr_constant, lambda: tf.math.divide(z,cmplx_pwr), lambda: z, name='Z_fixed')\n",
    "            #### AWGN ###\n",
    "            \n",
    "        with tf.name_scope('AWGN_Layer'):     \n",
    "            k=k.astype('float64')\n",
    "            #Converting SNR from db scale to linear scale\n",
    "            snr = 10**(self.snr_db/10.0)\n",
    "            snr = tf.dtypes.cast(snr, dtype='float64', name='Float32_64Cast')\n",
    "            ########### Calculating signal power ########### \n",
    "            #calculate absolute value of input\n",
    "            abs_val = tf.math.abs(Z, name='abs_val')\n",
    "            #Compute Square of all values and after that perform summation\n",
    "            summation = tf.math.reduce_sum(tf.math.square(abs_val, name='sq_awgn'), name='Summation')\n",
    "            #Computing signal power, dividing summantion by total number of values/symbols in a signal.\n",
    "            sig_pwr = tf.math.divide(summation,k, name='Signal_Pwr')\n",
    "            #Computing Noise power by dividing signal power by SNR.\n",
    "            noise_pwr = tf.math.divide(sig_pwr,snr, name='Noise_Pwr')\n",
    "            #Computing sigma for noise by taking sqrt of noise power and divide by two because our system is complex.\n",
    "            noise_sigma = tf.math.sqrt(noise_pwr/2, name='Noise_Sigma')\n",
    "\n",
    "            #creating the complex normal distribution.\n",
    "            z_img = tf.math.imag(Z, name = 'Z_imag')\n",
    "            z_real = tf.math.real(Z, name = 'Z_real')\n",
    "            rand_dist = tf.random.normal(tf.shape(z_real), dtype=tf.dtypes.float64, name='RandNormalDist')\n",
    "            #Compute product of sigma and complex normal distribution\n",
    "            noise = tf.math.multiply(noise_sigma, rand_dist, name='Noise')\n",
    "            #adding the awgn noise to the signal, noisy signal: ẑ\n",
    "            z_cap_Imag = tf.math.add(z_img, noise, name='z_cap_Imag') \n",
    "            z_cap_Imag = tf.dtypes.cast(z_cap_Imag, dtype='float32', name='NoisySignal_Imag')\n",
    "            \n",
    "            z_cap_Real = tf.math.add(z_real, noise, name='z_cap_Real') \n",
    "            z_cap_Real = tf.dtypes.cast(z_cap_Real, dtype='float32', name='NoisySignal_Real')\n",
    "            \n",
    "            return z_cap_Real  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "740a5dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCheckponitsHandler(tf.keras.callbacks.Callback):\n",
    "  def __init__(self, comp_ratio, snr_db, autoencoder, step):\n",
    "    super(ModelCheckponitsHandler, self).__init__()\n",
    "    self.comp_ratio = comp_ratio\n",
    "    self.snr_db = snr_db\n",
    "    self.step = step\n",
    "    self.autoencoder = autoencoder\n",
    "\n",
    "  def on_epoch_begin(self, epoch, logs=None):\n",
    "    if epoch%self.step==0:\n",
    "        os.makedirs('./CKPT_ByEpochs/CompRatio_'+str(self.comp_ratio)+'SNR'+str(self.snr_db), exist_ok=True)\n",
    "        path = './CKPT_ByEpochs/CompRatio_'+str(self.comp_ratio)+'SNR'+str(self.snr_db)+'/Autoencoder_Epoch_'+str(epoch)+'.h5'\n",
    "        self.autoencoder.save(path)\n",
    "        print('\\nModel Saved After {0} epochs.'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b2a04c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_filters(comp_ratio, F=5, n=3072):\n",
    "#지정된 압축률에 대해 마지막 컨볼루션 계층 및 첫 번째 트랜스포즈 컨볼루션 계층에 필요한 필터 수**\n",
    "\n",
    "    K = (comp_ratio*n)/F**2\n",
    "    return int(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fff36652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainAutoEncoder(x_train, x_test, nb_epoch, comp_ratio, batch_size, c, snr, saver_step=50):\n",
    "    ############################### Buliding Encoder ##############################\n",
    "    ''' Correspondance of different arguments w.r.t to literature: filters = K, kernel_size = FxF, strides = S'''\n",
    "    input_images = Input(shape=(32,32,3))\n",
    "    \n",
    "    #P = Input(shape=(), name='Power')\n",
    "    #snr_db = Input(shape=(), name='SNR_DB')\n",
    "    #1st convolutional layer\n",
    "    conv1 = Conv2D(filters=16, kernel_size=(5,5), strides=2, padding='valid', kernel_initializer='he_normal')(input_images)\n",
    "    prelu1 = PReLU()(conv1)\n",
    "    #2nd convolutional layer\n",
    "    conv2 = Conv2D(filters=80, kernel_size=(5,5), strides=2, padding='valid', kernel_initializer='he_normal')(prelu1)\n",
    "    prelu2 = PReLU()(conv2)\n",
    "    #3rd convolutional layer\n",
    "    conv3 = Conv2D(filters=50, kernel_size=(5,5), strides=1, padding='same', kernel_initializer='he_normal')(prelu2)\n",
    "    prelu3 = PReLU()(conv3)\n",
    "    #4th convolutional layer\n",
    "    conv4 = Conv2D(filters=40, kernel_size=(5,5), strides=1, padding='same', kernel_initializer='he_normal')(prelu3)\n",
    "    prelu4 = PReLU()(conv4)\n",
    "    #5th convolutional layer\n",
    "    conv5 = Conv2D(filters=c, kernel_size=(5,5), strides=1, padding='same', kernel_initializer='he_normal')(prelu4)\n",
    "    encoder = PReLU()(conv5)\n",
    "    \n",
    "    real_prod   = NormalizationNoise()(encoder)\n",
    "    \n",
    "    ############################### Building Decoder ##############################\n",
    "    #1st Deconvolutional layer\n",
    "    decoder = Conv2DTranspose(filters=40, kernel_size=(5,5), strides=1, padding='same', kernel_initializer='he_normal')(real_prod )\n",
    "    decoder = PReLU()(decoder)\n",
    "    #2nd Deconvolutional layer\n",
    "    decoder = Conv2DTranspose(filters=50, kernel_size=(5,5), strides=1, padding='same', kernel_initializer='he_normal')(decoder)\n",
    "    decoder = PReLU()(decoder)\n",
    "    #3rd Deconvolutional layer\n",
    "    decoder = Conv2DTranspose(filters=80, kernel_size=(5,5), strides=1, padding='same', kernel_initializer='he_normal')(decoder)\n",
    "    decoder = PReLU()(decoder)\n",
    "    #4th Deconvolutional layer\n",
    "    decoder = Conv2DTranspose(filters=16, kernel_size=(5,5), strides=2, padding='valid', kernel_initializer='he_normal')(decoder)\n",
    "    decoder = PReLU()(decoder)\n",
    "    #decoder_up = UpSampling2D((2,2))(decoder)\n",
    "    #5th Deconvolutional layer\n",
    "    decoder = Conv2DTranspose(filters=3, kernel_size=(5,5), strides=2, padding='valid', kernel_initializer='he_normal', activation ='sigmoid')(decoder)\n",
    "    #decoder = PReLU()(decoder)\n",
    "    decoder_up = UpSampling2D((2,2))(decoder)\n",
    "    decoder = Cropping2D(cropping=((13,13),(13,13)))(decoder_up)\n",
    "    \n",
    "    ############################### Buliding Models ###############################\n",
    "    autoencoder = Model(input_images, decoder)\n",
    "    \n",
    "    K.set_value(autoencoder.get_layer('normalization_noise_1').snr_db, snr)\n",
    "    autoencoder.compile(optimizer= Adam(learning_rate=0.0001), loss='mse', metrics=['accuracy'])\n",
    "    autoencoder.summary()\n",
    "    print('\\t-----------------------------------------------------------------')\n",
    "    print('\\t|\\t\\t\\t\\t\\t\\t\\t\\t|')\n",
    "    print('\\t|\\t\\t\\t\\t\\t\\t\\t\\t|')\n",
    "    print('\\t| Training Parameters: Filter Size: {0}, Compression ratio: {1} |'.format(c, comp_ratio))\n",
    "    print('\\t|\\t\\t\\t  SNR: {0} dB\\t\\t\\t\\t|'.format(snr))\n",
    "    print('\\t|\\t\\t\\t\\t\\t\\t\\t\\t|')\n",
    "    print('\\t|\\t\\t\\t\\t\\t\\t\\t\\t|')\n",
    "    print('\\t-----------------------------------------------------------------')\n",
    "    tb = keras.callbacks.tensorboard_v1.TensorBoard(log_dir='./Tensorboard/CompRatio{0}_SNR{1}'.format(str(comp_ratio), str(snr)))\n",
    "    os.makedirs('./checkpoints/CompRatio{0}_SNR{1}'.format(str(comp_ratio), str(snr)), exist_ok=True)\n",
    "    checkpoint = keras.callbacks.callbacks.ModelCheckpoint(filepath='./checkpoints/CompRatio{0}_SNR{1}'.format(str(comp_ratio), str(snr))+'/Autoencoder.h5', monitor='val_loss', save_best_only=True)\n",
    "    ckpt = ModelCheckponitsHandler(comp_ratio, snr, autoencoder, step=saver_step)\n",
    "    history = autoencoder.fit(x=x_train, y=x_train, batch_size=batch_size, epochs=nb_epoch,  callbacks=[tb, checkpoint, ckpt], validation_data=(x_test,x_test))\n",
    "    return history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
